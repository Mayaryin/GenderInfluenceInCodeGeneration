{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine Tune\n",
    "- RoBERTa\n",
    "- No need for inference speed up using distil bert since dataset is very small\n",
    "- Hyperparameter tuning using huggingfaces hyperparameter search\n",
    "- group k fold cross validation for prediction\n",
    "\n",
    "## Several conditions:\n",
    "- (spell corrected and) expanded prompts\n",
    "- raw conversational part\n"
   ],
   "id": "92df0da8972ba415"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:27.172771Z",
     "start_time": "2025-09-11T23:04:26.324542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ],
   "id": "26b5640ab14c7350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:28.070740Z",
     "start_time": "2025-09-11T23:04:28.046246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn  = sqlite3.connect('../../giicg.db')\n",
    "prompts = pd.read_sql(\"Select * from expanded_prompts\", conn)\n",
    "conn.close()\n",
    "prompts"
   ],
   "id": "134310018dd52364",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     message_id  conversation_id  role  \\\n",
       "0             1                1  user   \n",
       "1           730               32  user   \n",
       "2          1133               55  user   \n",
       "3          1135               55  user   \n",
       "4          1137               55  user   \n",
       "..          ...              ...   ...   \n",
       "748        1131               54  user   \n",
       "749        1532               71  user   \n",
       "750        1646               82  user   \n",
       "751        1849                2  user   \n",
       "752        1851                2  user   \n",
       "\n",
       "                                          message_text  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "748  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "749  from transformers import AutoTokenizer, AutoMo...   \n",
       "750  def run_query(query, n_results):\\n    query_em...   \n",
       "751  \\n    I am working on the problem of reconstru...   \n",
       "752  \\n        Focus on one step at the time. Write...   \n",
       "\n",
       "                                        conversational  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "748  I want to tune optimal thresholds. Currently, ...   \n",
       "749  I want to use an LLM for listwise reranking in...   \n",
       "750  this is my code. I want to: Get nodes and edge...   \n",
       "751  \\n    I am working on the problem of reconstru...   \n",
       "752  Focus on one step at the time. Write what is t...   \n",
       "\n",
       "                                                  code  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "748  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "749  from transformers import AutoTokenizer, AutoMo...   \n",
       "750  def run_query(query, n_results):\\n    query_em...   \n",
       "751                                                      \n",
       "752                                                      \n",
       "\n",
       "                                                 other           gender  \\\n",
       "0                                                       Man (cisgender)   \n",
       "1    report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...  Man (cisgender)   \n",
       "2                                                       Man (cisgender)   \n",
       "3                                                       Man (cisgender)   \n",
       "4     Transform given code to process large .mbox file  Man (cisgender)   \n",
       "..                                                 ...              ...   \n",
       "748  The narratives list looks like this:\\nnarrativ...  Man (cisgender)   \n",
       "749                                                     Man (cisgender)   \n",
       "750                                                     Man (cisgender)   \n",
       "751  Classic CV - Drone navigation\\nIf you ever tho...  Man (cisgender)   \n",
       "752                                                     Man (cisgender)   \n",
       "\n",
       "     user_id language  \n",
       "0          6       en  \n",
       "1          6       en  \n",
       "2          6       en  \n",
       "3          6       en  \n",
       "4          6       en  \n",
       "..       ...      ...  \n",
       "748       92       en  \n",
       "749       92       en  \n",
       "750       92       en  \n",
       "751        8       en  \n",
       "752        8       en  \n",
       "\n",
       "[753 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>role</th>\n",
       "      <th>message_text</th>\n",
       "      <th>conversational</th>\n",
       "      <th>code</th>\n",
       "      <th>other</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>user</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>730</td>\n",
       "      <td>32</td>\n",
       "      <td>user</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td></td>\n",
       "      <td>report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1133</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1135</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1137</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td></td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>1131</td>\n",
       "      <td>54</td>\n",
       "      <td>user</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>I want to tune optimal thresholds. Currently, ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>The narratives list looks like this:\\nnarrativ...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1532</td>\n",
       "      <td>71</td>\n",
       "      <td>user</td>\n",
       "      <td>from transformers import AutoTokenizer, AutoMo...</td>\n",
       "      <td>I want to use an LLM for listwise reranking in...</td>\n",
       "      <td>from transformers import AutoTokenizer, AutoMo...</td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1646</td>\n",
       "      <td>82</td>\n",
       "      <td>user</td>\n",
       "      <td>def run_query(query, n_results):\\n    query_em...</td>\n",
       "      <td>this is my code. I want to: Get nodes and edge...</td>\n",
       "      <td>def run_query(query, n_results):\\n    query_em...</td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1849</td>\n",
       "      <td>2</td>\n",
       "      <td>user</td>\n",
       "      <td>\\n    I am working on the problem of reconstru...</td>\n",
       "      <td>\\n    I am working on the problem of reconstru...</td>\n",
       "      <td></td>\n",
       "      <td>Classic CV - Drone navigation\\nIf you ever tho...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>8</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>1851</td>\n",
       "      <td>2</td>\n",
       "      <td>user</td>\n",
       "      <td>\\n        Focus on one step at the time. Write...</td>\n",
       "      <td>Focus on one step at the time. Write what is t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>8</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter and clean",
   "id": "c79c7fee0996934d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:32.189726Z",
     "start_time": "2025-09-11T23:04:30.236200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.normalization import remove_newlines\n",
    "\n",
    "prompts = prompts[prompts['gender'].isin(['Woman (cisgender)', 'Man (cisgender)'])].reset_index()\n",
    "prompts['conversational']  = prompts['conversational'].apply(remove_newlines)\n",
    "prompts"
   ],
   "id": "46bb067c87538e5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     index  message_id  conversation_id  role  \\\n",
       "0        0           1                1  user   \n",
       "1        1         730               32  user   \n",
       "2        2        1133               55  user   \n",
       "3        3        1135               55  user   \n",
       "4        4        1137               55  user   \n",
       "..     ...         ...              ...   ...   \n",
       "741    748        1131               54  user   \n",
       "742    749        1532               71  user   \n",
       "743    750        1646               82  user   \n",
       "744    751        1849                2  user   \n",
       "745    752        1851                2  user   \n",
       "\n",
       "                                          message_text  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "741  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "742  from transformers import AutoTokenizer, AutoMo...   \n",
       "743  def run_query(query, n_results):\\n    query_em...   \n",
       "744  \\n    I am working on the problem of reconstru...   \n",
       "745  \\n        Focus on one step at the time. Write...   \n",
       "\n",
       "                                        conversational  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "741  I want to tune optimal thresholds. Currently, ...   \n",
       "742  I want to use an LLM for listwise reranking in...   \n",
       "743  this is my code. I want to: Get nodes and edge...   \n",
       "744       I am working on the problem of reconstruc...   \n",
       "745  Focus on one step at the time. Write what is t...   \n",
       "\n",
       "                                                  code  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "741  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "742  from transformers import AutoTokenizer, AutoMo...   \n",
       "743  def run_query(query, n_results):\\n    query_em...   \n",
       "744                                                      \n",
       "745                                                      \n",
       "\n",
       "                                                 other           gender  \\\n",
       "0                                                       Man (cisgender)   \n",
       "1    report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...  Man (cisgender)   \n",
       "2                                                       Man (cisgender)   \n",
       "3                                                       Man (cisgender)   \n",
       "4     Transform given code to process large .mbox file  Man (cisgender)   \n",
       "..                                                 ...              ...   \n",
       "741  The narratives list looks like this:\\nnarrativ...  Man (cisgender)   \n",
       "742                                                     Man (cisgender)   \n",
       "743                                                     Man (cisgender)   \n",
       "744  Classic CV - Drone navigation\\nIf you ever tho...  Man (cisgender)   \n",
       "745                                                     Man (cisgender)   \n",
       "\n",
       "     user_id language  \n",
       "0          6       en  \n",
       "1          6       en  \n",
       "2          6       en  \n",
       "3          6       en  \n",
       "4          6       en  \n",
       "..       ...      ...  \n",
       "741       92       en  \n",
       "742       92       en  \n",
       "743       92       en  \n",
       "744        8       en  \n",
       "745        8       en  \n",
       "\n",
       "[746 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>message_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>role</th>\n",
       "      <th>message_text</th>\n",
       "      <th>conversational</th>\n",
       "      <th>code</th>\n",
       "      <th>other</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>user</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>730</td>\n",
       "      <td>32</td>\n",
       "      <td>user</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td></td>\n",
       "      <td>report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1133</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1135</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1137</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td></td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>748</td>\n",
       "      <td>1131</td>\n",
       "      <td>54</td>\n",
       "      <td>user</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>I want to tune optimal thresholds. Currently, ...</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>The narratives list looks like this:\\nnarrativ...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>749</td>\n",
       "      <td>1532</td>\n",
       "      <td>71</td>\n",
       "      <td>user</td>\n",
       "      <td>from transformers import AutoTokenizer, AutoMo...</td>\n",
       "      <td>I want to use an LLM for listwise reranking in...</td>\n",
       "      <td>from transformers import AutoTokenizer, AutoMo...</td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>750</td>\n",
       "      <td>1646</td>\n",
       "      <td>82</td>\n",
       "      <td>user</td>\n",
       "      <td>def run_query(query, n_results):\\n    query_em...</td>\n",
       "      <td>this is my code. I want to: Get nodes and edge...</td>\n",
       "      <td>def run_query(query, n_results):\\n    query_em...</td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>92</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>751</td>\n",
       "      <td>1849</td>\n",
       "      <td>2</td>\n",
       "      <td>user</td>\n",
       "      <td>\\n    I am working on the problem of reconstru...</td>\n",
       "      <td>I am working on the problem of reconstruc...</td>\n",
       "      <td></td>\n",
       "      <td>Classic CV - Drone navigation\\nIf you ever tho...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>8</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>752</td>\n",
       "      <td>1851</td>\n",
       "      <td>2</td>\n",
       "      <td>user</td>\n",
       "      <td>\\n        Focus on one step at the time. Write...</td>\n",
       "      <td>Focus on one step at the time. Write what is t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>8</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>746 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data stats and subsampling of long conversations\n",
    "- subsampled 50 prompts from user 73, who had over 200"
   ],
   "id": "334981c03b3f6aaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:34.120050Z",
     "start_time": "2025-09-11T23:04:34.109847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users_per_gender = prompts.groupby('gender')['user_id'].nunique().reset_index(name='num_users')\n",
    "users_per_gender"
   ],
   "id": "cc639a91cdc93756",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              gender  num_users\n",
       "0    Man (cisgender)         15\n",
       "1  Woman (cisgender)         12"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>num_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:44:04.816094Z",
     "start_time": "2025-09-11T22:44:04.763978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_per_user = prompts.groupby('user_id')['message_id'].nunique().reset_index(name='num_messages')\n",
    "messages_per_user"
   ],
   "id": "721f2c54de0aa65f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user_id  num_messages\n",
       "0         6             9\n",
       "1         8             2\n",
       "2        11            11\n",
       "3        15             3\n",
       "4        16            25\n",
       "5        25             4\n",
       "6        28            22\n",
       "7        31             5\n",
       "8        34            66\n",
       "9        46             5\n",
       "10       47            51\n",
       "11       48            16\n",
       "12       55            36\n",
       "13       56             6\n",
       "14       60             7\n",
       "15       63             2\n",
       "16       65            10\n",
       "17       73           229\n",
       "18       77            20\n",
       "19       79            61\n",
       "20       81             5\n",
       "21       83            15\n",
       "22       88             5\n",
       "23       89            31\n",
       "24       90            14\n",
       "25       91            81\n",
       "26       92             5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>79</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>91</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:39.044569Z",
     "start_time": "2025-09-11T23:04:39.029507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume your DataFrame is called `prompts`\n",
    "\n",
    "# 1. Separate out prompts for user 73 and other users\n",
    "user_73 = prompts[prompts['user_id'] == 73]\n",
    "other_users = prompts[prompts['user_id'] != 73]\n",
    "\n",
    "# 2. Randomly sample 50 prompts for user 73\n",
    "user_73_sampled = user_73.sample(n=50, random_state=42)\n",
    "\n",
    "# 3. Recombine\n",
    "prompts = pd.concat([other_users, user_73_sampled], ignore_index=True)\n",
    "\n",
    "subsampled_messages_per_user = prompts.groupby('user_id')['message_id'].nunique().reset_index(name='num_messages')\n",
    "subsampled_messages_per_user\n"
   ],
   "id": "fe36a322109b08f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    user_id  num_messages\n",
       "0         6             9\n",
       "1         8             2\n",
       "2        11            11\n",
       "3        15             3\n",
       "4        16            25\n",
       "5        25             4\n",
       "6        28            22\n",
       "7        31             5\n",
       "8        34            66\n",
       "9        46             5\n",
       "10       47            51\n",
       "11       48            16\n",
       "12       55            36\n",
       "13       56             6\n",
       "14       60             7\n",
       "15       63             2\n",
       "16       65            10\n",
       "17       73            50\n",
       "18       77            20\n",
       "19       79            61\n",
       "20       81             5\n",
       "21       83            15\n",
       "22       88             5\n",
       "23       89            31\n",
       "24       90            14\n",
       "25       91            81\n",
       "26       92             5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>79</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>91</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:50:34.771379Z",
     "start_time": "2025-09-11T22:50:34.756067Z"
    }
   },
   "cell_type": "code",
   "source": "prompts",
   "id": "16e79cf5dfe60aa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     index  message_id  conversation_id  role  \\\n",
       "0        0           1                1  user   \n",
       "1        1         730               32  user   \n",
       "2        2        1133               55  user   \n",
       "3        3        1135               55  user   \n",
       "4        4        1137               55  user   \n",
       "..     ...         ...              ...   ...   \n",
       "562    391        1234               65  user   \n",
       "563    429        1322               65  user   \n",
       "564    334         484               21  user   \n",
       "565    444        1364               65  user   \n",
       "566    348         516               21  user   \n",
       "\n",
       "                                          message_text  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "562             can we add peid for when pefile fails?   \n",
       "563  param_grid = {\\n    'min_samples': [5, 10, 20]...   \n",
       "564  i think i onlz want to think about the imbalan...   \n",
       "565  from sklearn.cluster import OPTICS\\nfrom sklea...   \n",
       "566  ah it did work.\\nnow i want to run:\\npredictor...   \n",
       "\n",
       "                                        conversational  \\\n",
       "0    parsing data from python iterator, how it coul...   \n",
       "1    Write python function to do operations with in...   \n",
       "2    Write shortest tutorial on creating RAG on ema...   \n",
       "3                                        what is FAISS   \n",
       "4     Transform given code to process large .mbox file   \n",
       "..                                                 ...   \n",
       "562             can we add peid for when pefile fails?   \n",
       "563                                 provide more steps   \n",
       "564  i think i only want to think about the imbalan...   \n",
       "565  this worked. but i do not have visualizations ...   \n",
       "566                 ah it did work. now i want to run:   \n",
       "\n",
       "                                                  code  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "562                                                      \n",
       "563  param_grid = {\\n    'min_samples': [5, 10, 20]...   \n",
       "564                                                      \n",
       "565  from sklearn.cluster import OPTICS\\nfrom sklea...   \n",
       "566  predictor.fit(\\n    train_data=train_data,  # ...   \n",
       "\n",
       "                                                 other             gender  \\\n",
       "0                                                         Man (cisgender)   \n",
       "1    report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...    Man (cisgender)   \n",
       "2                                                         Man (cisgender)   \n",
       "3                                                         Man (cisgender)   \n",
       "4     Transform given code to process large .mbox file    Man (cisgender)   \n",
       "..                                                 ...                ...   \n",
       "562                                                     Woman (cisgender)   \n",
       "563                                                     Woman (cisgender)   \n",
       "564                                                     Woman (cisgender)   \n",
       "565                                                     Woman (cisgender)   \n",
       "566  over each class 0 to 10 and create a model for...  Woman (cisgender)   \n",
       "\n",
       "     user_id language  \n",
       "0          6       en  \n",
       "1          6       en  \n",
       "2          6       en  \n",
       "3          6       en  \n",
       "4          6       en  \n",
       "..       ...      ...  \n",
       "562       73       en  \n",
       "563       73       en  \n",
       "564       73       en  \n",
       "565       73       en  \n",
       "566       73       en  \n",
       "\n",
       "[567 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>message_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>role</th>\n",
       "      <th>message_text</th>\n",
       "      <th>conversational</th>\n",
       "      <th>code</th>\n",
       "      <th>other</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>user</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td>parsing data from python iterator, how it coul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>730</td>\n",
       "      <td>32</td>\n",
       "      <td>user</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td>Write python function to do operations with in...</td>\n",
       "      <td></td>\n",
       "      <td>report_dt\\tsource\\tmetric_name\\tmetric_num\\tme...</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1133</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td>Write shortest tutorial on creating RAG on ema...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1135</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td>what is FAISS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1137</td>\n",
       "      <td>55</td>\n",
       "      <td>user</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td></td>\n",
       "      <td>Transform given code to process large .mbox file</td>\n",
       "      <td>Man (cisgender)</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>391</td>\n",
       "      <td>1234</td>\n",
       "      <td>65</td>\n",
       "      <td>user</td>\n",
       "      <td>can we add peid for when pefile fails?</td>\n",
       "      <td>can we add peid for when pefile fails?</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>73</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>429</td>\n",
       "      <td>1322</td>\n",
       "      <td>65</td>\n",
       "      <td>user</td>\n",
       "      <td>param_grid = {\\n    'min_samples': [5, 10, 20]...</td>\n",
       "      <td>provide more steps</td>\n",
       "      <td>param_grid = {\\n    'min_samples': [5, 10, 20]...</td>\n",
       "      <td></td>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>73</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>334</td>\n",
       "      <td>484</td>\n",
       "      <td>21</td>\n",
       "      <td>user</td>\n",
       "      <td>i think i onlz want to think about the imbalan...</td>\n",
       "      <td>i think i only want to think about the imbalan...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>73</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>444</td>\n",
       "      <td>1364</td>\n",
       "      <td>65</td>\n",
       "      <td>user</td>\n",
       "      <td>from sklearn.cluster import OPTICS\\nfrom sklea...</td>\n",
       "      <td>this worked. but i do not have visualizations ...</td>\n",
       "      <td>from sklearn.cluster import OPTICS\\nfrom sklea...</td>\n",
       "      <td></td>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>73</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>348</td>\n",
       "      <td>516</td>\n",
       "      <td>21</td>\n",
       "      <td>user</td>\n",
       "      <td>ah it did work.\\nnow i want to run:\\npredictor...</td>\n",
       "      <td>ah it did work. now i want to run:</td>\n",
       "      <td>predictor.fit(\\n    train_data=train_data,  # ...</td>\n",
       "      <td>over each class 0 to 10 and create a model for...</td>\n",
       "      <td>Woman (cisgender)</td>\n",
       "      <td>73</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create label mapping",
   "id": "b77ffb63791e49e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:42.371496Z",
     "start_time": "2025-09-11T23:04:42.364639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "labels = prompts['gender'].astype('category')\n",
    "prompts['label'] = labels.cat.codes\n",
    "label2id = dict(enumerate(labels.cat.categories))\n",
    "label2id\n",
    "\n",
    "\n",
    "with open(\"finetune/label2id.json\", \"w\") as f:\n",
    "    json.dump(label2id, f)\n",
    "\n"
   ],
   "id": "c78f4018b93015de",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Build dataset\n",
    "- group aware split: no prompts from the same user will occur in both sets\n",
    "- build dataset in huggingface format"
   ],
   "id": "d21fa90c937318b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:45.187985Z",
     "start_time": "2025-09-11T23:04:44.878483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import Dataset\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "groups = prompts['user_id']\n",
    "\n",
    "train_idx, val_idx = next(gss.split(prompts, groups=groups))\n",
    "train_prompts = prompts.iloc[train_idx]\n",
    "val_prompts = prompts.iloc[val_idx]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_prompts[['conversational', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_prompts[['conversational', 'label']])\n",
    "\n",
    "train_dataset"
   ],
   "id": "ab35febd8ab087c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversational', 'label', '__index_level_0__'],\n",
       "    num_rows: 450\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model, Tokenizer & Data Collator",
   "id": "910248eadf91a898"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:47.884598Z",
     "start_time": "2025-09-11T23:04:47.292620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def model_init():\n",
    "    # Needed for Trainer's hyperparameter search to re-initialize your model each trial\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"conversational\"],\n",
    "        truncation=True,\n",
    "        padding=False # padding is handled in the data collator\n",
    "    )\n"
   ],
   "id": "fbaf3d6a17d362e7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check max sample size",
   "id": "c83ef298cd798e25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:50.768155Z",
     "start_time": "2025-09-11T23:04:50.716007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Example: if your DataFrame is called user_prompts and the column is 'combined_prompts'\n",
    "# (Adjust to your actual variable/column names)\n",
    "texts = prompts['conversational'].tolist()\n",
    "\n",
    "# Count the tokens for each sample\n",
    "token_counts = [len(tokenizer.encode(text, add_special_tokens=True)) for text in texts]\n",
    "\n",
    "# Find the max, min, and average\n",
    "max_tokens = max(token_counts)\n",
    "min_tokens = min(token_counts)\n",
    "avg_tokens = sum(token_counts) / len(token_counts)\n",
    "\n",
    "print(f\"Max tokens: {max_tokens}\")\n",
    "print(f\"Min tokens: {min_tokens}\")\n"
   ],
   "id": "9824abc597d51e24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens: 407\n",
      "Min tokens: 4\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenize",
   "id": "db35919984043505"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:52.909511Z",
     "start_time": "2025-09-11T23:04:52.815723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset"
   ],
   "id": "99641e35b9519b91",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/450 [00:00<?, ? examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 20736.51 examples/s]\n",
      "Map: 100%|██████████| 117/117 [00:00<00:00, 16533.04 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversational', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 117\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trainer\n",
   "id": "72547bbbb809ac45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:04:56.054469Z",
     "start_time": "2025-09-11T23:04:55.232594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-6, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [4, 8])\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8, # finetune this\n",
    "    per_device_eval_batch_size=8, # finetune this\n",
    "    num_train_epochs=5, # finetune this\n",
    "    learning_rate=2e-5, # finetune this\n",
    "    #weight_decay= #\n",
    "    #warmup_steps = 10,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=50,         \n",
    "    logging_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ],
   "id": "5f5b00d0e2382b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/f5trsq6n2jx87f1mqgd_4vv80000gn/T/ipykernel_23921/1860672215.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search\n",
    "- optimizing for accuracy since classes are balanced (12:15)"
   ],
   "id": "d2f8ea28f9789749"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T23:06:22.067449Z",
     "start_time": "2025-09-11T23:04:58.701536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=10,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best_run.hyperparameters)\n",
    "print(\"Best eval accuracy:\", best_run.objective)\n"
   ],
   "id": "91bbc0ecaf26e2c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 01:04:58,747] A new study created in memory with name: no-name-16d99e4a-3b5c-4352-9508-686499bceb24\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 01:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638500</td>\n",
       "      <td>0.655973</td>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.688890</td>\n",
       "      <td>0.884848</td>\n",
       "      <td>0.632479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>1.265426</td>\n",
       "      <td>0.452991</td>\n",
       "      <td>0.508485</td>\n",
       "      <td>0.890598</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "[I 2025-09-12 01:06:09,741] Trial 0 finished with value: 0.452991452991453 and parameters: {'learning_rate': 2.2162344005696002e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}. Best is trial 0 with value: 0.452991452991453.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24/228 00:09 < 01:26, 2.35 it/s, Epoch 0.40/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-09-12 01:06:21,258] Trial 1 failed with parameters: {'learning_rate': 2.858253178699947e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8} because of the following error: RuntimeError('MPS backend out of memory (MPS allocated: 6.08 GiB, other allocations: 12.01 GiB, max allowed: 18.13 GiB). Tried to allocate 60.66 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/integrations/integration_utils.py\", line 274, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py\", line 2238, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py\", line 2582, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py\", line 3796, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py\", line 3884, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1194, in forward\n",
      "    outputs = self.roberta(\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 868, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 609, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 517, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 445, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 369, in forward\n",
      "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 6.08 GiB, other allocations: 12.01 GiB, max allowed: 18.13 GiB). Tried to allocate 60.66 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "[W 2025-09-12 01:06:21,263] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.08 GiB, other allocations: 12.01 GiB, max allowed: 18.13 GiB). Tried to allocate 60.66 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m best_run = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhyperparameter_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhp_space\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhp_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompute_objective\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meval_accuracy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBest hyperparameters:\u001B[39m\u001B[33m\"\u001B[39m, best_run.hyperparameters)\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBest eval accuracy:\u001B[39m\u001B[33m\"\u001B[39m, best_run.objective)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3674\u001B[39m, in \u001B[36mTrainer.hyperparameter_search\u001B[39m\u001B[34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001B[39m\n\u001B[32m   3671\u001B[39m \u001B[38;5;28mself\u001B[39m.hp_name = hp_name\n\u001B[32m   3672\u001B[39m \u001B[38;5;28mself\u001B[39m.compute_objective = default_compute_objective \u001B[38;5;28;01mif\u001B[39;00m compute_objective \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m compute_objective\n\u001B[32m-> \u001B[39m\u001B[32m3674\u001B[39m best_run = \u001B[43mbackend_obj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3676\u001B[39m \u001B[38;5;28mself\u001B[39m.hp_search_backend = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3677\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m best_run\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/hyperparameter_search.py:72\u001B[39m, in \u001B[36mOptunaBackend.run\u001B[39m\u001B[34m(self, trainer, n_trials, direction, **kwargs)\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer, n_trials: \u001B[38;5;28mint\u001B[39m, direction: \u001B[38;5;28mstr\u001B[39m, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrun_hp_search_optuna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:292\u001B[39m, in \u001B[36mrun_hp_search_optuna\u001B[39m\u001B[34m(trainer, n_trials, direction, **kwargs)\u001B[39m\n\u001B[32m    290\u001B[39m direction = \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m directions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m direction\n\u001B[32m    291\u001B[39m study = optuna.create_study(direction=direction, directions=directions, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_objective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    293\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m study._is_multi_objective():\n\u001B[32m    294\u001B[39m     best_trial = study.best_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/study.py:490\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    388\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    389\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    390\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    397\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    398\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    399\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    400\u001B[39m \n\u001B[32m    401\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    488\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    489\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m490\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m     frozen_trial_id = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:258\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    251\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    253\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    254\u001B[39m     updated_state == TrialState.FAIL\n\u001B[32m    255\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    256\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    257\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m258\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    259\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m trial._trial_id\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    199\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    200\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    202\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    203\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    204\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:274\u001B[39m, in \u001B[36mrun_hp_search_optuna.<locals>._objective\u001B[39m\u001B[34m(trial, checkpoint_dir)\u001B[39m\n\u001B[32m    272\u001B[39m     trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n\u001B[32m    273\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m274\u001B[39m     \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    275\u001B[39m \u001B[38;5;66;03m# If there hasn't been any evaluation during the training loop.\u001B[39;00m\n\u001B[32m    276\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(trainer, \u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2238\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2236\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2237\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2238\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2239\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2240\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2243\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2582\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2575\u001B[39m context = (\n\u001B[32m   2576\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2577\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2578\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2579\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2580\u001B[39m )\n\u001B[32m   2581\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2582\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2584\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2585\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2586\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2587\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2588\u001B[39m ):\n\u001B[32m   2589\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2590\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3796\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3793\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3795\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3796\u001B[39m     loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3798\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3799\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3800\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3801\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3802\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3884\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3882\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   3883\u001B[39m     inputs = {**inputs, **kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m3884\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3885\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   3886\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   3887\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1194\u001B[39m, in \u001B[36mRobertaForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1177\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1178\u001B[39m \u001B[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[32m   1179\u001B[39m \u001B[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1190\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1191\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1192\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1194\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1195\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1196\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1197\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1198\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1199\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1200\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1201\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1202\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1203\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1204\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1205\u001B[39m sequence_output = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1206\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.classifier(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:868\u001B[39m, in \u001B[36mRobertaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    861\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m    862\u001B[39m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[32m    863\u001B[39m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[32m    864\u001B[39m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[32m    865\u001B[39m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[32m    866\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m--> \u001B[39m\u001B[32m868\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    869\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    871\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    872\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    873\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    881\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    882\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:609\u001B[39m, in \u001B[36mRobertaEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    605\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m    607\u001B[39m layer_head_mask = head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m609\u001B[39m layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    610\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    611\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    612\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    613\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    614\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    615\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    616\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    620\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    621\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:517\u001B[39m, in \u001B[36mRobertaLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    506\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    507\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    508\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    515\u001B[39m     cache_position: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    516\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor]:\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    525\u001B[39m     attention_output = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    526\u001B[39m     outputs = self_attention_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add self attentions if we output attention weights\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:445\u001B[39m, in \u001B[36mRobertaAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    435\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    436\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    437\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    443\u001B[39m     cache_position: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    444\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor]:\n\u001B[32m--> \u001B[39m\u001B[32m445\u001B[39m     self_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    446\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    448\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    449\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    450\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    452\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    453\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    454\u001B[39m     attention_output = \u001B[38;5;28mself\u001B[39m.output(self_outputs[\u001B[32m0\u001B[39m], hidden_states)\n\u001B[32m    455\u001B[39m     outputs = (attention_output,) + self_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:369\u001B[39m, in \u001B[36mRobertaSdpaSelfAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    363\u001B[39m \u001B[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001B[39;00m\n\u001B[32m    364\u001B[39m \u001B[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001B[39;00m\n\u001B[32m    365\u001B[39m \u001B[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001B[39;00m\n\u001B[32m    366\u001B[39m \u001B[38;5;66;03m# a causal mask in case tgt_len == 1.\u001B[39;00m\n\u001B[32m    367\u001B[39m is_causal = \u001B[38;5;28mself\u001B[39m.is_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_cross_attention \u001B[38;5;129;01mand\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m tgt_len > \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m369\u001B[39m attn_output = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery_layer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkey_layer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue_layer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    374\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout_prob\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    376\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    378\u001B[39m attn_output = attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    379\u001B[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001B[38;5;28mself\u001B[39m.all_head_size)\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 6.08 GiB, other allocations: 12.01 GiB, max allowed: 18.13 GiB). Tried to allocate 60.66 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross Validation",
   "id": "b18154b7d4cc2f1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T22:02:03.827982Z",
     "start_time": "2025-08-17T21:58:15.637284Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()\n",
   "id": "591e7b302dd48833",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 03:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647527</td>\n",
       "      <td>0.652381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.832852</td>\n",
       "      <td>0.673810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>1.024136</td>\n",
       "      <td>0.647619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>1.189306</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>1.349580</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=210, training_loss=0.34968053102493285, metrics={'train_runtime': 227.9171, 'train_samples_per_second': 7.305, 'train_steps_per_second': 0.921, 'total_flos': 48587670260220.0, 'train_loss': 0.34968053102493285, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(trainer.evaluate())",
   "id": "a905404629d44754",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aba3e24c8fdb3278"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
